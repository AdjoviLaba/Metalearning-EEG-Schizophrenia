{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "T5nbpw6RooyW"
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gdown\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fN2dnz7ritjJ",
    "outputId": "2bfdae48-6c54-4a77-caa5-56bdebe53880"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in /home/adjovi/anaconda3/lib/python3.11/site-packages (5.2.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/adjovi/anaconda3/lib/python3.11/site-packages (from gdown) (4.12.3)\n",
      "Requirement already satisfied: filelock in /home/adjovi/anaconda3/lib/python3.11/site-packages (from gdown) (3.13.1)\n",
      "Requirement already satisfied: requests[socks] in /home/adjovi/anaconda3/lib/python3.11/site-packages (from gdown) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /home/adjovi/anaconda3/lib/python3.11/site-packages (from gdown) (4.66.5)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/adjovi/anaconda3/lib/python3.11/site-packages (from beautifulsoup4->gdown) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/adjovi/anaconda3/lib/python3.11/site-packages (from requests[socks]->gdown) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/adjovi/anaconda3/lib/python3.11/site-packages (from requests[socks]->gdown) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/adjovi/anaconda3/lib/python3.11/site-packages (from requests[socks]->gdown) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/adjovi/anaconda3/lib/python3.11/site-packages (from requests[socks]->gdown) (2024.8.30)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/adjovi/anaconda3/lib/python3.11/site-packages (from requests[socks]->gdown) (1.7.1)\n"
     ]
    },
    {
     "ename": "FileURLRetrievalError",
     "evalue": "Failed to retrieve file url:\n\n\tToo many users have viewed or downloaded this file recently. Please\n\ttry accessing the file again later. If the file you are trying to\n\taccess is particularly large or is shared with many people, it may\n\ttake up to 24 hours to be able to view or download the file. If you\n\tstill can't access a file after 24 hours, contact your domain\n\tadministrator.\n\nYou may still be able to access the file from the browser:\n\n\thttps://drive.google.com/uc?id=18cAvtcJc4jMkLi_QgA7vsN1oe2z026Dp\n\nbut Gdown can't. Please check connections and permissions.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileURLRetrievalError\u001b[0m                     Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gdown/download.py:267\u001b[0m, in \u001b[0;36mdownload\u001b[0;34m(url, output, quiet, proxy, speed, use_cookies, verify, id, fuzzy, resume, format, user_agent, log_messages)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 267\u001b[0m     url \u001b[38;5;241m=\u001b[39m get_url_from_gdrive_confirmation(res\u001b[38;5;241m.\u001b[39mtext)\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m FileURLRetrievalError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gdown/download.py:53\u001b[0m, in \u001b[0;36mget_url_from_gdrive_confirmation\u001b[0;34m(contents)\u001b[0m\n\u001b[1;32m     52\u001b[0m         error \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mgroups()[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 53\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m FileURLRetrievalError(error)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m url:\n",
      "\u001b[0;31mFileURLRetrievalError\u001b[0m: Too many users have viewed or downloaded this file recently. Please try accessing the file again later. If the file you are trying to access is particularly large or is shared with many people, it may take up to 24 hours to be able to view or download the file. If you still can't access a file after 24 hours, contact your domain administrator.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileURLRetrievalError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 13\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# To improve the speed of setting up, retrieve only the files for the two groups used in training\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Swap with the line below to get the entire dataset as a zip file\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# gdown.download('https://drive.google.com/uc?id=1-QTtycxsVNeym17zrMBSZCAAtEZEs05p', '/content/miniimagenet.zip', quiet=False)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Use the correct file ID from the Google Drive link\u001b[39;00m\n\u001b[1;32m     12\u001b[0m file_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m18cAvtcJc4jMkLi_QgA7vsN1oe2z026Dp\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 13\u001b[0m gdown\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://drive.google.com/uc?id=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminiimagenet.zip\u001b[39m\u001b[38;5;124m'\u001b[39m, quiet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# After downloading, check if the file exists\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gdown/download.py:278\u001b[0m, in \u001b[0;36mdownload\u001b[0;34m(url, output, quiet, proxy, speed, use_cookies, verify, id, fuzzy, resume, format, user_agent, log_messages)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m FileURLRetrievalError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    269\u001b[0m         message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    270\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to retrieve file url:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    271\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou may still be able to access the file from the browser:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    276\u001b[0m             url_origin,\n\u001b[1;32m    277\u001b[0m         )\n\u001b[0;32m--> 278\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m FileURLRetrievalError(message)\n\u001b[1;32m    280\u001b[0m filename_from_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    281\u001b[0m last_modified_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mFileURLRetrievalError\u001b[0m: Failed to retrieve file url:\n\n\tToo many users have viewed or downloaded this file recently. Please\n\ttry accessing the file again later. If the file you are trying to\n\taccess is particularly large or is shared with many people, it may\n\ttake up to 24 hours to be able to view or download the file. If you\n\tstill can't access a file after 24 hours, contact your domain\n\tadministrator.\n\nYou may still be able to access the file from the browser:\n\n\thttps://drive.google.com/uc?id=18cAvtcJc4jMkLi_QgA7vsN1oe2z026Dp\n\nbut Gdown can't. Please check connections and permissions."
     ]
    }
   ],
   "source": [
    "!pip install gdown\n",
    "\n",
    "import gdown\n",
    "\n",
    "\n",
    "# To improve the speed of setting up, retrieve only the files for the two groups used in training\n",
    "# Swap with the line below to get the entire dataset as a zip file\n",
    "# gdown.download('https://drive.google.com/uc?id=1-QTtycxsVNeym17zrMBSZCAAtEZEs05p', '/content/miniimagenet.zip', quiet=False)\n",
    "\n",
    "\n",
    "# Use the correct file ID from the Google Drive link\n",
    "file_id = '18cAvtcJc4jMkLi_QgA7vsN1oe2z026Dp'\n",
    "gdown.download(f'https://drive.google.com/uc?id={file_id}', 'miniimagenet.zip', quiet=False)\n",
    "\n",
    "# After downloading, check if the file exists\n",
    "import os\n",
    "if os.path.exists('miniimagenet.zip'):\n",
    "    print(\"File downloaded successfully.\")\n",
    "else:\n",
    "    print(\"File download failed.\")\n",
    "\n",
    "images_directory = '/content/miniimagenet/images/'\n",
    "if not os.path.exists(images_directory):\n",
    "  os.makedirs(images_directory)\n",
    "!unzip -qq /content/miniimagenet.zip -d {images_directory}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4IlR8VhnkHWZ",
    "outputId": "54f4834a-b790-4981-fb63-4c0707d6f7d3"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# get the CSV with the list of all file names\n",
    "mini_imagenet_file_list_csv = '/content/all_imagenet_file_names.csv'\n",
    "\n",
    "\n",
    "# Download the CSV file from Google Drive using the file ID\n",
    "mini_imagenet_file_list_csv = '/content/all_imagenet_file_names.csv'\n",
    "file_id = '1oSpNEBEfvsxqwRJshIfi1QnHXlurdkbe'\n",
    "gdown.download(f'https://drive.google.com/uc?id={file_id}', mini_imagenet_file_list_csv, quiet=False)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "file_list = pd.read_csv(mini_imagenet_file_list_csv)\n",
    "\n",
    "# Check the first few rows\n",
    "print(file_list.head())\n",
    "\n",
    "# Rest of your code\n",
    "selected_groups = ['n01532829', 'n01558993']\n",
    "samples_miniimagenet = file_list[file_list['label'].isin(selected_groups)].groupby('label').first()\n",
    "\n",
    "print(samples_miniimagenet)\n",
    "\n",
    "\n",
    "#selected_groups = ['n01532829', 'n01558993']\n",
    "#samples_miniimagenet = file_list[file_list['label'].isin(selected_groups)].groupby('label').first()\n",
    "\n",
    "\n",
    "# Other options for group pairs\n",
    "# n01532829, n01558993\n",
    "# n02108551, n02108915"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 774
    },
    "id": "QpJTxKj01lkC",
    "outputId": "7ff75927-592a-47dd-9300-72e9acdae9b6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from importlib import reload\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Base directory where the images are stored\n",
    "images_directory = '/content/miniimagenet/images/'\n",
    "\n",
    "# Check the columns in samples_miniimagenet\n",
    "print(samples_miniimagenet.columns)\n",
    "\n",
    "# Loop through the rows of samples_miniimagenet\n",
    "for label, row in samples_miniimagenet.iterrows():\n",
    "    image_path = row['filename']\n",
    "    print(f'Sample for group ID: {label}')\n",
    "\n",
    "    # Construct the full image path, adding the label as a subdirectory\n",
    "    full_image_path = os.path.join(images_directory, label, image_path)\n",
    "    print(f\"Trying to load: {full_image_path}\")\n",
    "\n",
    "    # Check for the file and display it if found\n",
    "    if os.path.exists(full_image_path):\n",
    "        print(f\"Image found: {full_image_path}\")\n",
    "\n",
    "        # Reload and display the image\n",
    "        img = mpimg.imread(full_image_path)\n",
    "        reload(plt)\n",
    "        imgplot = plt.imshow(img)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"Image not found for {full_image_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XGPtV09KSzPI"
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "For this analysis, the first two groups above are used n01532829 and n01558993. These are two similar-looking bird species. The goal is to train the classifier to detect minor details that distinguish the two groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lW6hGHlVUd2a"
   },
   "outputs": [],
   "source": [
    "# Since the imagenet files are all in the same directory, they can be used as-is\n",
    "# with the maml-pytorch setup and do not need to be processed further at this point.\n",
    "# However, the directory does need to be added to the Python training file\n",
    "\n",
    "file_list[file_list['label'].isin(selected_groups)].to_csv(images_directory + '/train.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iMEsTbSjKGXF",
    "outputId": "558ac6c8-316d-4c28-89ac-a4a411f27c77"
   },
   "outputs": [],
   "source": [
    "#Get the EEG spectrograms zip file and unzip it\n",
    "eeg_image_directory = '/content/eeg_sz_spectrograms'\n",
    "gdown.download('https://drive.google.com/uc?id=1WZ1yIFE2bng0McnY_4UBJHqTttsXTTNX', '{}.zip'.format(eeg_image_directory), quiet=False)\n",
    "!unzip -qq {eeg_image_directory}.zip -d {eeg_image_directory}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3jU4_ON9NLFJ"
   },
   "outputs": [],
   "source": [
    "# rename\n",
    "dl_link = '/content/eeg_sz_spectrograms/gen_data_20s_70pct_overlap_-_high_nfft_all_channels_sml/'\n",
    "!mv \"{dl_link}/hc\" {eeg_image_directory}\n",
    "!mv \"{dl_link}/sz\" {eeg_image_directory}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UkZkXEI-USMt",
    "outputId": "2d82cc6e-d694-4cc8-d5db-d71ce2cdcfce"
   },
   "outputs": [],
   "source": [
    "# Use EEG of Sz for validation and testing\n",
    "# Extract files from an eeg_sz spectrogram directory where files are saved by subject\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "rand_seed = 1\n",
    "# List of raw patient file IDs that should be skipped based on categorization as outliers\n",
    "ignore_list = ['h09', 'h10', 's10', 's11', 's12']\n",
    "hc_subject_ids = ['hc' + str(i) for i in range(14) if \"h{:02}\".format(i) not in ignore_list]\n",
    "sz_subject_ids = ['sz' + str(i) for i in range(14) if \"s{:02}\".format(i) not in ignore_list]\n",
    "all_subject_ids = np.concatenate([hc_subject_ids, sz_subject_ids], axis=0)\n",
    "validate_hc, test_hc = train_test_split(hc_subject_ids, test_size=0.5, random_state=rand_seed)\n",
    "validate_sz, test_sz = train_test_split(sz_subject_ids, test_size=0.5, random_state=rand_seed)\n",
    "\n",
    "validation_ids = np.concatenate([validate_hc, validate_sz])\n",
    "test_ids = np.concatenate([test_hc, test_sz])\n",
    "\n",
    "\n",
    "print('\\nSubjects assigned to groups using sklearn.model_selection.train_test_split')\n",
    "print('Test group: ', \", \".join(test_ids), \"\\n\")\n",
    "print('Validation group: ', \", \".join(validation_ids), \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "from shutil import copyfile\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "test_images_output_directory = 'all_test_images'\n",
    "validation_images_output_directory = 'all_validation_images'\n",
    "\n",
    "if not os.path.exists(test_images_output_directory):\n",
    "    os.mkdir(test_images_output_directory)\n",
    "if not os.path.exists(validation_images_output_directory):\n",
    "    os.mkdir(validation_images_output_directory)\n",
    "\n",
    "\n",
    "\n",
    "# Note: CSV is only used for MAML and Prototypical networks\n",
    "def gen_csv_and_copy_sz_files(image_dir, img_output_dir, participant_ids, output_name, split_with_csv=False):\n",
    "    subdir_data = []\n",
    "    for group in ['hc', 'sz']:  # ['Healthy_Control', 'Sz_Patient']\n",
    "        group_dir = os.path.join(image_dir, group)\n",
    "        print(f\"Looking in group directory: {group_dir}\")\n",
    "\n",
    "        for pid in os.listdir(group_dir):  # by participant IDs\n",
    "            print(f\"Found folder: {pid}\")\n",
    "\n",
    "            # Ensure the folder name matches one of the participant IDs\n",
    "            if pid in participant_ids:\n",
    "                print(f\"Copying files for participant ID: {pid}\")\n",
    "                participant_dir = os.path.join(group_dir, pid)\n",
    "\n",
    "                # Ensure the output directory exists\n",
    "                output_group_dir = os.path.join(img_output_dir, group)\n",
    "                if not os.path.exists(output_group_dir):\n",
    "                    os.makedirs(output_group_dir)\n",
    "\n",
    "                for file in os.listdir(participant_dir):\n",
    "                    file_data = {'filename': file, 'label': group}\n",
    "                    subdir_data.append(file_data)\n",
    "\n",
    "                    # Construct destination path\n",
    "                    destination = os.path.join(output_group_dir, file) if split_with_csv else '{}/{}/{}'.format(img_output_dir, group, file)\n",
    "                    copyfile(os.path.join(participant_dir, file), destination)\n",
    "\n",
    "    if split_with_csv:\n",
    "        pd.DataFrame(subdir_data).to_csv(os.path.join(img_output_dir, output_name))\n",
    "\n",
    "    return pd.DataFrame(subdir_data)\n",
    "\n",
    "\n",
    "\n",
    "df = gen_csv_and_copy_sz_files(image_dir=eeg_image_directory,\n",
    "                                img_output_dir=test_images_output_directory,\n",
    "                                participant_ids=test_ids,\n",
    "                               split_with_csv=True,\n",
    "                                output_name= 'test.csv')\n",
    "df = gen_csv_and_copy_sz_files(image_dir=eeg_image_directory,\n",
    "                                img_output_dir=validation_images_output_directory,\n",
    "                                participant_ids=validation_ids,\n",
    "                               split_with_csv=True,\n",
    "                                output_name= 'test.csv') #file must be name test.csv\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6iuCAgmsQxc6",
    "outputId": "33a43303-772c-441a-f685-e5eb48c910d7"
   },
   "outputs": [],
   "source": [
    "# Get the project files from github\n",
    "!git clone https://github.com/MTynes/MAML-Pytorch.git maml_pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EKft9PEXeKVl",
    "outputId": "c6e0f490-2494-41c8-b9d6-92751eefeca7"
   },
   "outputs": [],
   "source": [
    "!python /content/maml_pytorch/train_custom_dataset.py --help\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CjY6ePw7wxfP",
    "outputId": "847d3105-452e-4339-f847-b5f6a14b00be"
   },
   "outputs": [],
   "source": [
    "# run the training file\n",
    "\n",
    "import timeit\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "n_epochs = 400 * 10000 # must be a multiple of 10000\n",
    "\n",
    "train_dir = '/content/miniimagenet/images'\n",
    "!python /content/maml_pytorch/train_custom_dataset.py  --epochs {n_epochs} --run_further_training 'false'\n",
    "\n",
    "\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('MAML execution time: {} hrs'.format((stop - start)/60/60) )\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s4yuJ0RCUqkk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "id": "NTnveARvCEoz",
    "outputId": "8858948f-4f61-4b31-ad5b-7f0ce01c97d6"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "\n",
    "print('Mean Validation Accuracy over Epochs')\n",
    "\n",
    "text_file = open('/content/mean_test_accuracy.txt', \"r\")\n",
    "mean_accs = text_file.read().split('\\n')\n",
    "mean_accs = [(float(ma) * 100) for ma in mean_accs]\n",
    "\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([np.int(min(mean_accs)-2), np.int(max(mean_accs)) +2])\n",
    "\n",
    "plt.plot(mean_accs)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Accuracy')\n",
    "plt.title('Mean Validation Accuracy over Epochs')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z8CNM0cudewf"
   },
   "outputs": [],
   "source": [
    "metrics = pd.read_csv('mean_metrics.csv')\n",
    "metrics.head()\n",
    "\n",
    "\n",
    "metrics[['train_loss', 'val_loss']].plot(figsize=(10,5), title='Train and Validation Loss over Epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v8yA0OmweYW2"
   },
   "outputs": [],
   "source": [
    "metrics[['train_accuracy', 'val_accuracy']].plot(figsize=(10,5), title='Train and Validation Accuracy over Epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pnJ94L5X3rCu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Us5RosXM44wR"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "# modified from main.py https://github.com/zhangrong1722/CheXNet-Pytorch\n",
    "\n",
    "def plt_roc(test_y, probas_y, plot_micro=False, plot_macro=False):\n",
    "    assert isinstance(test_y, list) and isinstance(probas_y, list), 'the type of input must be list'\n",
    "    skplt.metrics.plot_roc(test_y, probas_y, plot_micro=plot_micro, plot_macro=plot_macro)\n",
    "    plt.savefig('roc_auc_curve.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "###########################################\n",
    "# Define confusion matrix and ROC visualization functions\n",
    "# from https://colab.research.google.com/drive/1ISfhxFDntfOos7cOeT7swduSqzLEqyFn#scrollTo=UiKRYOWPfhJs\n",
    "\n",
    "def plot_confusion_matrix(cm, classes=None,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues,\n",
    "                          cv=10):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"\\nNormalized confusion matrix\")\n",
    "    else:\n",
    "        print('\\nConfusion matrix, without normalization')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    if classes:\n",
    "        tick_marks = np.arange(len(classes))\n",
    "        plt.xticks(tick_marks, classes, rotation=45)\n",
    "        plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 1.5\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.locator_params(nbins=2)\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cPCU0a2g3OhG"
   },
   "outputs": [],
   "source": [
    "pred_df = pd.read_csv('test_predictions_and_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aTReVLrh4Th_"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "# Get the predicted labels for metric calculations.\n",
    "#\n",
    "# Since prototypical networks always sets the ground truth to 0,\n",
    "# infer the predicted class label from true label and the Boolean value for 'correct'\n",
    "# To do this, set hc to be -1 and sz to 1. This allows the opposite class to be selected\n",
    "# for rows where correct is False by multiplying the true_label by -1\n",
    "pred_df['true_label'] = pred_df.apply(lambda x: -1 if x['true_label'] == 0 else 1, axis=1)\n",
    "pred_df['correct'] = pred_df.apply(lambda x: 1 if x['correct'] == True else 0, axis=1)\n",
    "pred_df['prediction'] = pred_df.apply(lambda x: x['true_label']\n",
    "  if x['correct'] == 1 else x['true_label'] * -1, axis=1)\n",
    "pred_df.replace(-1, 0, inplace=True)\n",
    "# display(pred_df.head())\n",
    "# pred_df.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rn3KjBgT4baC"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "pred_y = pred_df['prediction'].values\n",
    "truth_y = pred_df['true_label'].values\n",
    "#probas_y = [s.replace('[', '').replace(']', '').split(', ') for s in best_model_preds['probas_y'].values]\n",
    "#probas_y = [[float(t[0]), float(t[1])] for t in probas_y]\n",
    "\n",
    "\n",
    "confusion = confusion_matrix(pred_y, truth_y)\n",
    "plot_confusion_matrix(confusion,\n",
    "                      classes=['hc', 'sz'],\n",
    "                      title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FTBiitsce0fn"
   },
   "outputs": [],
   "source": [
    "pd.read_csv('metrics_summary.csv')\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
